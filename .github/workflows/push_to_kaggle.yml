

name: CI
on:
  push:
    branches: [ "main" ]

  workflow_dispatch:

  schedule:
    - cron: "33 03 * * 6"
    - cron: "33 03 * * 3"

env:
  DATASET_URL: maxisoft/binance-futures-stats

jobs:
  push_kaggle:

    runs-on: ubuntu-latest

    steps:

      - uses: actions/checkout@v3
        with:
          fetch-depth: 2

      - name: Kaggle Login
        timeout-minutes: 5
        uses: osbm/kaggle-login@v2.3
        with:
          KAGGLE_USERNAME: maxisoft
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}

      - name: Download dataset metadata
        timeout-minutes: 1
        run: |
          chmod 600 ~/.kaggle/kaggle.json
          mkdir -p binancedumper
          pushd binancedumper 
          kaggle datasets metadata ${DATASET_URL}
          popd
          
      - name: Download dataset
        timeout-minutes: 90
        run: |
          mkdir -p downloading
          pushd downloading
          kaggle datasets download --unzip ${DATASET_URL}
          [ -d "binancedumper" ] && pushd binancedumper
          rm -f state.json || :
          rm -f pairs.json || :
          rm -rf markprice_*.csv || :
          rm -rf btcusdt_depth20_*.csv || :
          ls -lah
          [ -d "../binancedumper" ] && popd
          popd
          mv --no-clobber downloading/* binancedumper
          rm -rf downloading

      - name: LONG running binance-dumper via docker
        timeout-minutes: 60
        run: |
          chown -R 1001 binancedumper
          timeout --preserve-status --kill-after=1000 1600 docker run --rm -v `pwd`/binancedumper:/binancedumper ghcr.io/maxisoft/binance-dumper/binance-dumper:latest || :
      
      - name: Upload dataset
        timeout-minutes: 90
        run: |
          kaggle datasets version -p binancedumper -r zip --delete-old-versions -m "update on ${EPOCHSECONDS}"
